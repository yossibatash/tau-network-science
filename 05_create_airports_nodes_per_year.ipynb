{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this Spark ETL process we are creating dim Airplanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "spark = SparkSession.builder.appName('AirportsNodes').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load airports data set\n",
    "\n",
    "# Airports \n",
    "airports_file = f'{cwd}/data/raw_data/labels/airports.csv'\n",
    "adf = spark.read.csv(path=airports_file, header=True)\n",
    "adf.createOrReplaceTempView('ADF')\n",
    "# adf.printSchema()\n",
    "\n",
    "# airports.csv (downloaded from - https://ourairports.com/data/).\n",
    "# Schema:\n",
    "# root\n",
    "#  |-- id: string (nullable = true)\n",
    "#  |-- ident: string (nullable = true)\n",
    "#  |-- type: string (nullable = true)\n",
    "#  |-- name: string (nullable = true)\n",
    "#  |-- latitude_deg: string (nullable = true)\n",
    "#  |-- longitude_deg: string (nullable = true)\n",
    "#  |-- elevation_ft: string (nullable = true)\n",
    "#  |-- continent: string (nullable = true)\n",
    "#  |-- iso_country: string (nullable = true)\n",
    "#  |-- iso_region: string (nullable = true)\n",
    "#  |-- municipality: string (nullable = true)\n",
    "#  |-- scheduled_service: string (nullable = true)\n",
    "#  |-- gps_code: string (nullable = true)\n",
    "#  |-- iata_code: string (nullable = true)\n",
    "#  |-- local_code: string (nullable = true)\n",
    "#  |-- home_link: string (nullable = true)\n",
    "#  |-- wikipedia_link: string (nullable = true)\n",
    "#  |-- keywords: string (nullable = true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Load reduced flights data (tau-network-science/data/stg/flights_each_day_week_14_data)\n",
    "\n",
    "file = f'{cwd}/data/stg/flights_each_day_week_14_data/*.csv'\n",
    "fdf = spark.read.csv(path=file, header=True)\n",
    "fdf = fdf.withColumn(\"filename\", input_file_name())\n",
    "fdf.createOrReplaceTempView('FDF')\n",
    "# fdf.printSchema()\n",
    "\n",
    "# flightlist_*.csv.gz (downloaded from - https://zenodo.org/record/4601479#.YLaEU5MzYUH).\n",
    "# Schema:\n",
    "# root\n",
    "#  |-- flight_date: string (nullable = true)\n",
    "#  |-- flight_dayofweek: string (nullable = true)\n",
    "#  |-- airplane_code: string (nullable = true)\n",
    "#  |-- airport_source_code: string (nullable = true)\n",
    "#  |-- airport_target_code: string (nullable = true)\n",
    "#  |-- flights_count: string (nullable = true)\n",
    "#  |-- flight_year: string (nullable = true)\n",
    "#  |-- filename: string (nullable = false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Create distinct list of active airports during week 14 in years 2019|2020|2021\n",
    "# In the end our nodes will be countries and the edges international flights between countries.\n",
    "# So first we are creating a new data set from airports.csv and countries.csv (joined by iso_country code)\n",
    "distinct_airports = spark.sql(\"\"\"\n",
    "\n",
    "select  airport_source_code as airport_Id, flight_year as flight_year\n",
    "from    FDF\n",
    "union\n",
    "select  airport_target_code as airport_Id, flight_year as flight_year\n",
    "from    FDF\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "distinct_airports.createOrReplaceTempView('distinct_airports')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_iso_2_code: string (nullable = true)\n",
      " |-- report_year: string (nullable = true)\n",
      " |-- country_iso_3_code: string (nullable = true)\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- continent_name: string (nullable = true)\n",
      " |-- country_lat: string (nullable = true)\n",
      " |-- country_lng: string (nullable = true)\n",
      " |-- total_cases: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- cases_population_ratio: string (nullable = true)\n",
      " |-- covid19_percentage: string (nullable = true)\n",
      " |-- covid19_percentage_label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load countries and covid19 data set\n",
    "countries_file = f'{cwd}/data/stg/covid19/countries_covid_19_week_14_agg.csv'\n",
    "cdf = spark.read.csv(path=countries_file, header=True)\n",
    "cdf.createOrReplaceTempView('countries_covid19')\n",
    "cdf.printSchema()\n",
    "\n",
    "# Schema:\n",
    "# root\n",
    "#  |-- country_iso_2_code: string (nullable = true)\n",
    "#  |-- c19_report_year: string (nullable = true)\n",
    "#  |-- total_cases: string (nullable = true)\n",
    "#  |-- population: string (nullable = true)\n",
    "#  |-- avg_cases_population_ratio: string (nullable = true)\n",
    "#  |-- country_iso_3_code: string (nullable = true)\n",
    "#  |-- country_name: string (nullable = true)\n",
    "#  |-- continent_name: string (nullable = true)\n",
    "#  |-- country_lat: string (nullable = true)\n",
    "#  |-- country_lng: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the end our nodes will be countries and the edges international flights between countries.\n",
    "# Create a new data set from airports and countries_covid19 (joined by country_iso_2_code code)\n",
    "airports = spark.sql(\"\"\"\n",
    "select  distinct \n",
    "        A.ident              as airport_id,\n",
    "        A.name               as airport_name,\n",
    "        A.type               as airport_type,\n",
    "        A.latitude_deg       as airport_lat,\n",
    "        A.longitude_deg      as airport_lng,\n",
    "        C.country_iso_2_code as country_iso_2_code,\n",
    "        C.country_iso_3_code as country_iso_3_code,\n",
    "        C.country_name       as country_name,  \n",
    "        C.continent_name     as continent_name,                \n",
    "        C.country_lat        as country_lat,        \n",
    "        C.country_lng        as country_lng,\n",
    "        C.total_cases        as cases_per_week_14,\n",
    "        C.population         as population,\n",
    "        C.cases_population_ratio as covid19_percentage,\n",
    "        cast(cast(C.cases_population_ratio as decimal(3, 2))as string) || \"%\" as covid19_percentage_label,\n",
    "        C.report_year as report_year,\n",
    "        C.report_year as year        \n",
    "from    ADF as A \n",
    "        JOIN \n",
    "        distinct_airports as D\n",
    "        ON A.ident = D.airport_Id\n",
    "        left join\n",
    "        countries_covid19 as C \n",
    "        ON A.iso_country = C.country_iso_2_code\n",
    "           and D.flight_year = C.report_year\n",
    "\"\"\")\n",
    "airports.createOrReplaceTempView('airports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports.repartition(1).write \\\n",
    ".partitionBy('year') \\\n",
    ".mode('overwrite') \\\n",
    ".format(\"com.databricks.spark.csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"quoteAll\", \"true\") \\\n",
    ".save(\"./data/nodes/airports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ./data/nodes/airports/*2019*/*.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Copy new file into proper data folder location\n",
    "!cp ./data/nodes/airports/*2019*/*.csv ./data/nodes/airports/airports_nodes_2019.csv\n",
    "# Delete Spark output folder\n",
    "!rm -rf ./data/nodes/airports/*2019*/\n",
    "# Check that the folder deleted\n",
    "!ls ./data/nodes/airports/*2019*/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ./data/nodes/airports/*2020*/*.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Copy new file into proper data folder location\n",
    "!cp ./data/nodes/airports/*2020*/*.csv ./data/nodes/airports/airports_nodes_2020.csv\n",
    "# Delete Spark output folder\n",
    "!rm -rf ./data/nodes/airports/*2020*/\n",
    "# Check that the folder deleted\n",
    "!ls ./data/nodes/airports/*2020*/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ./data/nodes/airports/*2021*/*.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Copy new file into proper data folder location\n",
    "!cp ./data/nodes/airports/*2021*/*.csv ./data/nodes/airports/airports_nodes_2021.csv\n",
    "# Delete Spark output folder\n",
    "!rm -rf ./data/nodes/airports/*2021*/\n",
    "# Check that the folder deleted\n",
    "!ls ./data/nodes/airports/*2021*/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "# adf.count()      #65,562\n",
    "# airports.count() #12,503\n",
    "\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "\n",
    "# select  count(*) as row_count,\n",
    "#         cast(sum( case when  airport_id                             is not null then 1 else 0 end) as double)/count(*) *100 as NN_airport_id,\n",
    "#         cast(sum( case when  airport_name                           is not null then 1 else 0 end) as double)/count(*) *100 as NN_airport_name,\n",
    "#         cast(sum( case when  airport_type                           is not null then 1 else 0 end) as double)/count(*) *100 as NN_airport_type,\n",
    "#         cast(sum( case when  airport_lat                            is not null then 1 else 0 end) as double)/count(*) *100 as NN_airport_lat,\n",
    "#         cast(sum( case when  airport_lng                            is not null then 1 else 0 end) as double)/count(*) *100 as NN_airport_lng,\n",
    "#         cast(sum( case when  country_iso_2_code                     is not null then 1 else 0 end) as double)/count(*) *100 as NN_country_iso_2_code,\n",
    "#         cast(sum( case when  country_iso_3_code                     is not null then 1 else 0 end) as double)/count(*) *100 as NN_country_iso_3_code,\n",
    "#         cast(sum( case when  country_name                           is not null then 1 else 0 end) as double)/count(*) *100 as NN_country_name,\n",
    "#         cast(sum( case when  country_lat                            is not null then 1 else 0 end) as double)/count(*) *100 as NN_country_lat,\n",
    "#         cast(sum( case when  country_lng                            is not null then 1 else 0 end) as double)/count(*) *100 as NN_country_lng,\n",
    "#         cast(sum( case when  avg_cases_per_week_14                  is not null then 1 else 0 end) as double)/count(*) *100 as NN_avg_cases_per_week_14,\n",
    "#         cast(sum( case when  population                             is not null then 1 else 0 end) as double)/count(*) *100 as NN_population,\n",
    "#         cast(sum( case when  avg_cases_population_ratio_per_week_14 is not null then 1 else 0 end) as double)/count(*) *100 as NN_avg_cases_population_ratio_per_week_14\n",
    "# from    airports\n",
    "\n",
    "\n",
    "# \"\"\").show(10,False)\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "\n",
    "# select  avg_cases_population_ratio_per_week_14,\n",
    "#         covid19_percentage_label\n",
    "# from    airports\n",
    "\n",
    "\n",
    "# \"\"\").show(10,False)\n",
    "\n",
    "\n",
    "\n",
    "# No null values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ./airports/*.csv\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_test_env)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
