{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this Spark ETL process we are creating nodes and adges files (from the OpenSky data set) \n",
    "# for creating our network in Gephi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "spark = SparkSession.builder.appName('GraphData').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Load files with labels/attributes for Nodes enrichment \n",
    "\n",
    "# Airports \n",
    "airports_file = f'{cwd}/data/raw_data/labels/airports.csv'\n",
    "adf = spark.read.csv(path=airports_file, header=True)\n",
    "adf.createOrReplaceTempView('ADF')\n",
    "# adf.printSchema()\n",
    "\n",
    "# airports.csv (downloaded from - https://ourairports.com/data/).\n",
    "# Schema:\n",
    "# root\n",
    "#  |-- id: string (nullable = true)\n",
    "#  |-- ident: string (nullable = true)\n",
    "#  |-- type: string (nullable = true)\n",
    "#  |-- name: string (nullable = true)\n",
    "#  |-- latitude_deg: string (nullable = true)\n",
    "#  |-- longitude_deg: string (nullable = true)\n",
    "#  |-- elevation_ft: string (nullable = true)\n",
    "#  |-- continent: string (nullable = true)\n",
    "#  |-- iso_country: string (nullable = true)\n",
    "#  |-- iso_region: string (nullable = true)\n",
    "#  |-- municipality: string (nullable = true)\n",
    "#  |-- scheduled_service: string (nullable = true)\n",
    "#  |-- gps_code: string (nullable = true)\n",
    "#  |-- iata_code: string (nullable = true)\n",
    "#  |-- local_code: string (nullable = true)\n",
    "#  |-- home_link: string (nullable = true)\n",
    "#  |-- wikipedia_link: string (nullable = true)\n",
    "#  |-- keywords: string (nullable = true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries\n",
    "countries_file = f'{cwd}/data/raw_data/labels/countries.csv'\n",
    "cdf = spark.read.csv(path=countries_file, header=True)\n",
    "cdf.createOrReplaceTempView('CDF')\n",
    "# cdf.printSchema()\n",
    "\n",
    "# countries.csv (downloaded from - https://github.com/google/dspl/blob/master/samples/google/canonical/countries.csv).\n",
    "# Schema:\n",
    "# root\n",
    "#  |-- country: string (nullable = true)\n",
    "#  |-- latitude: string (nullable = true)\n",
    "#  |-- longitude: string (nullable = true)\n",
    "#  |-- name: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Load Edges data (OpenSky data)\n",
    "\n",
    "file = f'{cwd}/data/raw_data/flights/flightlist_*.csv.gz'\n",
    "fdf = spark.read.csv(path=file, header=True)\n",
    "fdf = fdf.withColumn(\"filename\", input_file_name())\n",
    "fdf.createOrReplaceTempView('FDF')\n",
    "# fdf.printSchema()\n",
    "\n",
    "# flightlist_*.csv.gz (downloaded from - https://zenodo.org/record/4601479#.YLaEU5MzYUH).\n",
    "# Schema:\n",
    "# root\n",
    "#  |-- callsign: string (nullable = true)\n",
    "#  |-- number: string (nullable = true)\n",
    "#  |-- icao24: string (nullable = true)\n",
    "#  |-- registration: string (nullable = true)\n",
    "#  |-- typecode: string (nullable = true)\n",
    "#  |-- origin: string (nullable = true)\n",
    "#  |-- destination: string (nullable = true)\n",
    "#  |-- firstseen: string (nullable = true)\n",
    "#  |-- lastseen: string (nullable = true)\n",
    "#  |-- day: string (nullable = true)\n",
    "#  |-- latitude_1: string (nullable = true)\n",
    "#  |-- longitude_1: string (nullable = true)\n",
    "#  |-- altitude_1: string (nullable = true)\n",
    "#  |-- latitude_2: string (nullable = true)\n",
    "#  |-- longitude_2: string (nullable = true)\n",
    "#  |-- altitude_2: string (nullable = true)\n",
    "#  |-- filename: string (nullable = false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the end our nodes will be countries and the edges international flights between countries.\n",
    "# So first we are creating a new data set from airports.csv and countries.csv (joined by iso_country code)\n",
    "\n",
    "nodes_df = spark.sql(\"\"\"\n",
    "select  A.ident         as Airport_Id,\n",
    "        A.name          as Airport_Name,\n",
    "        A.type          as Airport_Type,\n",
    "        A.latitude_deg  as Airport_lat,\n",
    "        A.longitude_deg as Airport_lng,\n",
    "        C.country       as Country_Id,\n",
    "        C.name          as Country_Name,        \n",
    "        C.latitude      as Country_latitude,        \n",
    "        C.longitude     as Country_longitude        \n",
    "from    ADF as A JOIN CDF as C \n",
    "        ON A.iso_country = C.country\n",
    "\"\"\")\n",
    "\n",
    "nodes_df.createOrReplaceTempView('NDF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Countries (Nodes) CSV file\n",
    "countries = spark.sql(\"\"\"\n",
    "select  distinct Country_Id as Id,\n",
    "        Country_Name        as Label,        \n",
    "        Country_latitude    as lat,        \n",
    "        Country_longitude   as lng        \n",
    "FROM    NDF\n",
    "\"\"\")\n",
    "\n",
    "countries.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"nodes_countries\")\n",
    "\n",
    "\n",
    "# Copy new file into proper data folder location\n",
    "!cp ./nodes_countries/*.csv ./data/nodes/nodes_countries.csv\n",
    "# Delete Spark output folder\n",
    "!rm -rf ./nodes_countries\n",
    "# Check that the folder deleted\n",
    "!ls ./nodes_countries/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------+-------------+------------------+-------------------+----------+-------------+----------------+-----------------+\n",
      "|Airport_Id|Airport_Name                      |Airport_Type |Airport_lat       |Airport_lng        |Country_Id|Country_Name |Country_latitude|Country_longitude|\n",
      "+----------+----------------------------------+-------------+------------------+-------------------+----------+-------------+----------------+-----------------+\n",
      "|00A       |Total Rf Heliport                 |heliport     |40.07080078125    |-74.93360137939453 |US        |United States|37.09024        |-95.712891       |\n",
      "|00AA      |Aero B Ranch Airport              |small_airport|38.704022         |-101.473911        |US        |United States|37.09024        |-95.712891       |\n",
      "|00AK      |Lowell Field                      |small_airport|59.94919968       |-151.695999146     |US        |United States|37.09024        |-95.712891       |\n",
      "|00AL      |Epps Airpark                      |small_airport|34.86479949951172 |-86.77030181884766 |US        |United States|37.09024        |-95.712891       |\n",
      "|00AR      |Newport Hospital & Clinic Heliport|closed       |35.6087           |-91.254898         |US        |United States|37.09024        |-95.712891       |\n",
      "|00AS      |Fulton Airport                    |small_airport|34.9428028        |-97.8180194        |US        |United States|37.09024        |-95.712891       |\n",
      "|00AZ      |Cordes Airport                    |small_airport|34.305599212646484|-112.16500091552734|US        |United States|37.09024        |-95.712891       |\n",
      "|00CA      |Goldstone (GTS) Airport           |small_airport|35.35474          |-116.885329        |US        |United States|37.09024        |-95.712891       |\n",
      "|00CL      |Williams Ag Airport               |small_airport|39.427188         |-121.763427        |US        |United States|37.09024        |-95.712891       |\n",
      "|00CN      |Kitchen Creek Helibase Heliport   |heliport     |32.7273736        |-116.4597417       |US        |United States|37.09024        |-95.712891       |\n",
      "+----------+----------------------------------+-------------+------------------+-------------------+----------+-------------+----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nodes_df.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each query aggregate one month flights data into a data fream, we are:\n",
    "# Counting for each route the number of flights per month\n",
    "# filtering incomplate filght rows (missing one of the origin/destination attribute)\n",
    "# filtering domestic flights\n",
    "\n",
    "def preper_edges_query(f_year, f_month):\n",
    "    query = f\"\"\"With stg \n",
    "                as (select origin                   as Source,\n",
    "                           destination              as Target,\n",
    "                           count(*)                 as Weight \n",
    "                    from   FDF       \n",
    "                    where  1=1\n",
    "                           and origin is not null and destination is not null\n",
    "                           and origin<>destination\n",
    "                           and filename = 'file:///Users/ybatash/PycharmProjects/jupyter/tau-network-science/data/raw_data/flights/flightlist_{f_year}_{f_month}.csv.gz'\n",
    "                    group by 1,2)               \n",
    "                \n",
    "                select  S.Country_Id as Source,\n",
    "                        T.Country_Id as Target,        \n",
    "                        Sum(E.Weight)as Weight\n",
    "                from    stg E join NDF S\n",
    "                        on E.Source = S.Airport_Id\n",
    "                        join NDF T\n",
    "                        on E.Target = T.Airport_Id\n",
    "                group by 1,2\"\"\"\n",
    "    return spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year - 2019 Month - 01\n",
      "Year - 2019 Month - 02\n",
      "Year - 2019 Month - 03\n",
      "Year - 2019 Month - 04\n",
      "Year - 2020 Month - 01\n",
      "Year - 2020 Month - 02\n",
      "Year - 2020 Month - 03\n",
      "Year - 2020 Month - 04\n",
      "Year - 2021 Month - 01\n",
      "Year - 2021 Month - 02\n"
     ]
    }
   ],
   "source": [
    "for year in list(['2019','2020','2021']):\n",
    "    for month in list(['01','02','03','04']):\n",
    "        res_df = None\n",
    "        if year=='2021' and month=='03':\n",
    "            break\n",
    "        print(f'Year - {year} Month - {month}')\n",
    "        res_df = preper_edges_query(f_year=year, f_month=month)\n",
    "        res_df.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(f'flights_{year}_{month}')\n",
    "        \n",
    "        folder_path = f'{cwd}/flights_{year}_{month}'\n",
    "        file_path = f'{folder_path}/*.csv'\n",
    "        to_path = f'{cwd}/data/edges/flights_{year}_{month}.csv'\n",
    "        # Copy new file into proper data folder location\n",
    "        !cp $file_path $to_path\n",
    "        # Delete Spark output folder\n",
    "        !rm -rf $folder_path\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_path = f'{cwd}/flights_{year}_{month}'\n",
    "# file_path = f'{folder_path}/*.csv'\n",
    "# to_path = f'{cwd}/data/edges/flights_{year}_{month}.csv'\n",
    "# # Copy new file into proper data folder location\n",
    "# !cp $file_path $to_path\n",
    "# # Delete Spark output folder\n",
    "# !rm -rf $folder_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_between_countries = spark.sql(\"\"\"\n",
    "# With edges\n",
    "# as( select  E.Source       as Source,\n",
    "#             S.Country_Id as S_Country_Id,\n",
    "#             E.Target       as Target,\n",
    "#             T.Country_Id as T_Country_Id,        \n",
    "#             Weight\n",
    "#     from   EDF E join NDF S\n",
    "#             on E.Source = S.Airport_Id\n",
    "#            join NDF T\n",
    "#             on E.Target = T.Airport_Id)\n",
    "\n",
    "# select  S_Country_Id as Source,\n",
    "#         T_Country_Id as Target,        \n",
    "#         Sum(Weight)    as Weight\n",
    "# from    edges\n",
    "# where   S_Country_Id<>T_Country_Id\n",
    "# group by 1,2\n",
    "            \n",
    "# \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flight_between_countries.show(10,False)\n",
    "\n",
    "# flight_between_countries.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"flight_between_countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_edges = '/Users/ybatash/PycharmProjects/jupyter/tau-network-science/b_01_2019/flight_between_countries.csv'\n",
    "# tdf = spark.read.csv(path=tmp_edges, header=True)\n",
    "# tdf.createOrReplaceTempView('TDF')\n",
    "# tdf.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries = spark.sql(\"\"\"\n",
    "# With Con_filter\n",
    "# as ( select  Source as Country_Id\n",
    "#      from    TDF\n",
    "#      union\n",
    "#      select  Target as Country_Id\n",
    "#      from    TDF)\n",
    "     \n",
    "# select  distinct N.Country_Id as Id,\n",
    "#         N.Country_Name as Label,        \n",
    "#         N.Country_latitude as lat,        \n",
    "#         N.Country_longitude as lng        \n",
    "# FROM    NDF as N join Con_filter as F\n",
    "#         on N.Country_Id=F.Country_Id\n",
    "     \n",
    "# \"\"\")\n",
    "\n",
    "# countries.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"countries\")\n",
    "\n",
    "# # nodes_df.createOrReplaceTempView('NDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes_df = \n",
    "# spark.sql(\"\"\"\n",
    "# select  A.ident         as Airport_Id,\n",
    "#         A.name          as Airport_Name,\n",
    "#         A.type          as Airport_Type,\n",
    "#         A.latitude_deg  as Airport_lat,\n",
    "#         A.longitude_deg as Airport_lng,\n",
    "#         C.country       as Country_Id,\n",
    "#         C.name          as Country_Name,        \n",
    "#         C.latitude      as Country_latitude,        \n",
    "#         C.longitude     as Country_longitude        \n",
    "# from    ADF as A JOIN CDF as C \n",
    "#         ON A.iso_country = C.country\n",
    "# \"\"\")\n",
    "\n",
    "# nodes_df.createOrReplaceTempView('NDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"b_01_2019\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# select origin                   as origin_airport,\n",
    "#        destination              as destination_airport,\n",
    "#        count(*)                 as flights_count,\n",
    "#        count(distinct callsign) as distinct_callsign_count,\n",
    "#        count(distinct icao24)   as distinct_icao24_count,\n",
    "#        min(firstseen)           as min_firstseen,\n",
    "#        max(firstseen)           as max_firstseen \n",
    "# from   T       \n",
    "# where   1=1\n",
    "#         and origin is not null and destination  is not null\n",
    "#         and filename = 'file:///Users/ybatash/PycharmProjects/jupyter/tau-network-science/data/raw_data/flightlist_2019_01.csv.gz'\n",
    "# group by 1,2        \n",
    "# \"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_test_env)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
