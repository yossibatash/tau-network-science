{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this Spark ETL process we are creating dim Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "spark = SparkSession.builder.appName('DimCountries').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countries\n",
    "countries_file = f'{cwd}/data/raw_data/labels/countries.csv'\n",
    "cdf = spark.read.csv(path=countries_file, header=True)\n",
    "cdf.createOrReplaceTempView('CDF')\n",
    "# cdf.printSchema()\n",
    "\n",
    "# countries.csv (downloaded from - https://github.com/google/dspl/blob/master/samples/google/canonical/countries.csv).\n",
    "# Schema:\n",
    "# root\n",
    "#  |-- country: string (nullable = true)\n",
    "#  |-- latitude: string (nullable = true)\n",
    "#  |-- longitude: string (nullable = true)\n",
    "#  |-- name: string (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country: string (nullable = true)\n",
      " |-- iso_2_code: string (nullable = true)\n",
      " |-- iso_3_code: string (nullable = true)\n",
      " |-- numeric: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2 - Load the iso 2 to 3 lookup\n",
    "\n",
    "# Covid19 \n",
    "file = f'{cwd}/data/raw_data/covid19/countries_iso_2_to3_lkp.csv'\n",
    "lkp = spark.read.csv(path=file, header=True)\n",
    "lkp.createOrReplaceTempView('LKP')\n",
    "lkp.printSchema()\n",
    "\n",
    "# owid-covid-data.csv (downloaded from - https://github.com/owid/covid-19-data/blob/master/public/data/README.md).\n",
    "# Schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- continent_name: string (nullable = true)\n",
      " |-- continent_code: string (nullable = true)\n",
      " |-- iso_2_code: string (nullable = true)\n",
      " |-- iso_3_code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Load continent label for each country\n",
    "\n",
    "# Covid19 \n",
    "file = f'{cwd}/data/raw_data/labels/continents.csv'\n",
    "lkp2 = spark.read.csv(path=file, header=True)\n",
    "lkp2.createOrReplaceTempView('LKP2')\n",
    "lkp2.printSchema()\n",
    "\n",
    "# owid-covid-data.csv (downloaded from - https://github.com/owid/covid-19-data/blob/master/public/data/README.md).\n",
    "# Schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = spark.sql(\"\"\"\n",
    "select  distinct\n",
    "        C.country       as country_id,\n",
    "        L2.continent_name as continent_name,\n",
    "        L.iso_3_code    as country_iso_3_code,\n",
    "        C.name          as country_name,        \n",
    "        C.latitude      as country_lat,        \n",
    "        C.longitude     as country_lng  \n",
    "from    CDF C left join LKP L\n",
    "        ON C.country = L.iso_2_code\n",
    "        left join LKP2 L2\n",
    "        ON C.country = L2.iso_2_code\n",
    "where   1=1\n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ./countries/*.csv\r\n"
     ]
    }
   ],
   "source": [
    "countries.repartition(1).write \\\n",
    ".format(\"com.databricks.spark.csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"quoteAll\", \"true\") \\\n",
    ".save(\"countries\")\n",
    "\n",
    "# Copy new file into proper data folder location\n",
    "!cp ./countries/*.csv ./data/dims/dim_countries.csv\n",
    "# Delete Spark output folder\n",
    "!rm -rf ./countries\n",
    "# Check that the folder deleted\n",
    "!ls ./countries/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_test_env)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
