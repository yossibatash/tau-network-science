{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- callsign: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- icao24: string (nullable = true)\n",
      " |-- registration: string (nullable = true)\n",
      " |-- typecode: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      " |-- firstseen: string (nullable = true)\n",
      " |-- lastseen: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- latitude_1: string (nullable = true)\n",
      " |-- longitude_1: string (nullable = true)\n",
      " |-- altitude_1: string (nullable = true)\n",
      " |-- latitude_2: string (nullable = true)\n",
      " |-- longitude_2: string (nullable = true)\n",
      " |-- altitude_2: string (nullable = true)\n",
      " |-- filename: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.appName('loadRawData').getOrCreate()\n",
    "file = '/Users/ybatash/PycharmProjects/jupyter/tau-network-science/data/raw_data/flightlist_*.csv.gz'\n",
    "df = spark.read.csv(path=file, header=True)\n",
    "df = df.withColumn(\"filename\", input_file_name())\n",
    "df.createOrReplaceTempView('T')\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# select \n",
    "#         count(*) as row_count,\n",
    "#         sum( case when callsign     is not null then 1 else 0 end)/count(*) * 100 as nn_count_callsign     ,\n",
    "#         sum( case when number       is not null then 1 else 0 end)/count(*) * 100 as nn_count_number       ,\n",
    "#         sum( case when icao24       is not null then 1 else 0 end)/count(*) * 100 as nn_count_icao24       ,\n",
    "#         sum( case when registration is not null then 1 else 0 end)/count(*) * 100 as nn_count_registration ,\n",
    "#         sum( case when typecode     is not null then 1 else 0 end)/count(*) * 100 as nn_count_typecode     ,\n",
    "#         sum( case when origin       is not null then 1 else 0 end)/count(*) * 100 as nn_count_origin       ,\n",
    "#         sum( case when destination  is not null then 1 else 0 end)/count(*) * 100 as nn_count_destination  ,\n",
    "\n",
    "#        sum( case\n",
    "#                   when origin       is not null and\n",
    "#                        destination  is not null\n",
    "#                     then 1 else 0 end)/count(*) * 100 as nn_count_origin_destination       ,\n",
    "\n",
    "\n",
    "#         sum( case when firstseen    is not null then 1 else 0 end)/count(*) * 100 as nn_count_firstseen    ,\n",
    "#         sum( case when lastseen     is not null then 1 else 0 end)/count(*) * 100 as nn_count_lastseen     ,\n",
    "#         sum( case when day          is not null then 1 else 0 end)/count(*) * 100 as nn_count_day          ,\n",
    "#         sum( case when latitude_1   is not null then 1 else 0 end)/count(*) * 100 as nn_count_latitude_1   ,\n",
    "#         sum( case when longitude_1  is not null then 1 else 0 end)/count(*) * 100 as nn_count_longitude_1  ,\n",
    "#         sum( case when altitude_1   is not null then 1 else 0 end)/count(*) * 100 as nn_count_altitude_1   ,\n",
    "#         sum( case when latitude_2   is not null then 1 else 0 end)/count(*) * 100 as nn_count_latitude_2   ,\n",
    "#         sum( case when longitude_2  is not null then 1 else 0 end)/count(*) * 100 as nn_count_longitude_2  ,\n",
    "#         sum( case when altitude_2   is not null then 1 else 0 end)/count(*) * 100 as nn_count_altitude_2\n",
    "# from T\n",
    "# where origin is not null and destination  is not null\n",
    "# \"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"\n",
    "# select \n",
    "#         filename,\n",
    "#         count(*) as row_count,\n",
    "#         sum( case when callsign     is not null then 1 else 0 end)/count(*) * 100 as nn_count_callsign     ,\n",
    "#         sum( case when number       is not null then 1 else 0 end)/count(*) * 100 as nn_count_number       ,\n",
    "#         sum( case when icao24       is not null then 1 else 0 end)/count(*) * 100 as nn_count_icao24       ,\n",
    "#         sum( case when registration is not null then 1 else 0 end)/count(*) * 100 as nn_count_registration ,\n",
    "#         sum( case when typecode     is not null then 1 else 0 end)/count(*) * 100 as nn_count_typecode     ,\n",
    "#         sum( case when origin       is not null then 1 else 0 end)/count(*) * 100 as nn_count_origin       ,\n",
    "#         sum( case when destination  is not null then 1 else 0 end)/count(*) * 100 as nn_count_destination  ,\n",
    "\n",
    "#        sum( case\n",
    "#                   when origin       is not null and\n",
    "#                        destination  is not null\n",
    "#                     then 1 else 0 end)/count(*) * 100 as nn_count_origin_destination       ,\n",
    "\n",
    "\n",
    "#         sum( case when firstseen    is not null then 1 else 0 end)/count(*) * 100 as nn_count_firstseen    ,\n",
    "#         sum( case when lastseen     is not null then 1 else 0 end)/count(*) * 100 as nn_count_lastseen     ,\n",
    "#         sum( case when day          is not null then 1 else 0 end)/count(*) * 100 as nn_count_day          ,\n",
    "#         sum( case when latitude_1   is not null then 1 else 0 end)/count(*) * 100 as nn_count_latitude_1   ,\n",
    "#         sum( case when longitude_1  is not null then 1 else 0 end)/count(*) * 100 as nn_count_longitude_1  ,\n",
    "#         sum( case when altitude_1   is not null then 1 else 0 end)/count(*) * 100 as nn_count_altitude_1   ,\n",
    "#         sum( case when latitude_2   is not null then 1 else 0 end)/count(*) * 100 as nn_count_latitude_2   ,\n",
    "#         sum( case when longitude_2  is not null then 1 else 0 end)/count(*) * 100 as nn_count_longitude_2  ,\n",
    "#         sum( case when altitude_2   is not null then 1 else 0 end)/count(*) * 100 as nn_count_altitude_2\n",
    "# from T\n",
    "# where origin is not null and destination  is not null\n",
    "# group by 1\n",
    "# \"\"\").show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = spark.sql(\"\"\"\n",
    "select origin                   as Source,\n",
    "       destination              as Target,\n",
    "       count(*)                 as Weight \n",
    "from   T       \n",
    "where   1=1\n",
    "        and origin is not null and destination  is not null\n",
    "        and filename like 'file:///Users/ybatash/PycharmProjects/jupyter/tau-network-science/data/raw_data/flightlist_2019_%.csv.gz'\n",
    "group by 1,2        \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMALL EDGES FILE\n",
    "\n",
    "# res_df = spark.sql(\"\"\"\n",
    "# select origin                   as Source,\n",
    "#        destination              as Target,\n",
    "#        count(*)                 as Weight \n",
    "# from   T       \n",
    "# where   1=1\n",
    "#         and origin is not null and destination  is not null\n",
    "#         and filename like 'file:///Users/ybatash/PycharmProjects/jupyter/tau-network-science/data/raw_data/flightlist_2019_01.csv.gz'\n",
    "# group by 1,2        \n",
    "# \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"b_01_2019\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------+-----------------------+---------------------+-------------------------+-------------------------+\n",
      "|origin_airport|destination_airport|flights_count|distinct_callsign_count|distinct_icao24_count|min_firstseen            |max_firstseen            |\n",
      "+--------------+-------------------+-------------+-----------------------+---------------------+-------------------------+-------------------------+\n",
      "|01FA          |KLKR               |2            |2                      |2                    |2019-01-06 22:34:07+00:00|2019-01-09 15:36:47+00:00|\n",
      "|05IN          |KORD               |1            |1                      |1                    |2019-01-31 22:13:31+00:00|2019-01-31 22:13:31+00:00|\n",
      "|05MD          |KGAI               |1            |1                      |1                    |2019-01-31 20:34:36+00:00|2019-01-31 20:34:36+00:00|\n",
      "|06NC          |94NC               |1            |1                      |1                    |2019-01-08 20:22:32+00:00|2019-01-08 20:22:32+00:00|\n",
      "|08TE          |9TX5               |1            |1                      |1                    |2019-01-23 23:42:47+00:00|2019-01-23 23:42:47+00:00|\n",
      "|0FL9          |KX58               |1            |1                      |1                    |2019-01-05 20:50:35+00:00|2019-01-05 20:50:35+00:00|\n",
      "|0OK5          |KPWA               |1            |1                      |1                    |2019-01-24 17:14:40+00:00|2019-01-24 17:14:40+00:00|\n",
      "|11TX          |9XS4               |1            |1                      |1                    |2019-01-07 22:47:58+00:00|2019-01-07 22:47:58+00:00|\n",
      "|13CL          |KSAC               |2            |2                      |2                    |2019-01-11 23:06:05+00:00|2019-01-19 22:46:02+00:00|\n",
      "|13CL          |KVCB               |2            |1                      |1                    |2019-01-10 20:09:54+00:00|2019-01-24 22:09:07+00:00|\n",
      "+--------------+-------------------+-------------+-----------------------+---------------------+-------------------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select origin                   as origin_airport,\n",
    "       destination              as destination_airport,\n",
    "       count(*)                 as flights_count,\n",
    "       count(distinct callsign) as distinct_callsign_count,\n",
    "       count(distinct icao24)   as distinct_icao24_count,\n",
    "       min(firstseen)           as min_firstseen,\n",
    "       max(firstseen)           as max_firstseen \n",
    "from   T       \n",
    "where   1=1\n",
    "        and origin is not null and destination  is not null\n",
    "        and filename = 'file:///Users/ybatash/PycharmProjects/jupyter/tau-network-science/data/raw_data/flightlist_2019_01.csv.gz'\n",
    "group by 1,2        \n",
    "\"\"\").show(10, False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_test_env)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
